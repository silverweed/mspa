%%%%%% 2: DESCRIZIONE ADABOOST

\section{Adaboost}
Adaboost ({\it Adaptive Boosting}) \`e un ``meta-algoritmo'' usato per costruire in modo incrementale un classificatore forte a partire da un numero $T$ di classificatori deboli ({\it weak learner}). La potenza di Adaboost \`e data in gran parte dall'essere un metodo generico, ovvero dal non dipendere in modo sostanziale dalla tipologia di {\it weak learner} utilizzata\footnote{l'unica richiesta \`e che il {\it weak learner} sia in grado di produrre risultati leggermente migliori di una selezione casuale.}. Per questa ragione, si sceglie spesso di utilizzare {\it weak learner} molto semplici, in modo da minimizzare il peso computazionale dell'algoritmo.

Nel presente progetto utilizziamo dei {\it weak learner} della forma
\begin{center}
	\( h^i_{p, \tau}(x) = sgn(x_p - \tau), \hskip 2em 0 \le \tau \le 255, \hskip 0.5em 0 \le p < 784\footnote{\(784 = 28 \times 28\), pari alla dimensione delle immagini considerate come array di pixel.} \)
\end{center}
dove \(x = \{x_0, \dotsc, x_{783}\}\) \`e un'immagine del dataset e $p$ e $\tau$ sono parametri che rappresentano rispettivamente l'indice del pixel da considerare e la soglia sulla scurezza di tale pixel.

Una volta scelti i classificatori base \({h_i}\), Adaboost produce un classificatore forte che ha la forma

\begin{equation} \label{eq:strongclass}
H(x) \eqdef sgn(f(x)), \hskip 2em f(x) = \sum\limits_{i=1}^T{w_i h^i(x)}
\end{equation}

Andiamo cos\`i a costruire incrementalmente un classificatore forte di immagini aggiungendo man mano test binari su singoli pixel scelti accuratamente in modo da massimizzare l'efficacia di tali test.

Dal punto di vista matematico, ``massimizzare l'efficacia'' di un test corrisponde a scegliere di volta in volta il classificatore \(h_{p,\tau}^i\) che ha il minimo training error possibile tra quelli a disposizione. Per la precisione si minimizza un maggiorante convesso del training error, che andremo ora a definire.

\hfill \break
Per prima cosa si introducono le funzioni
 
 \[ L_i(t) \eqdef h_i(x_t)y_t \hskip 2em  i = 1, \dotsc, T \]
 
le quali, data un'immagine $x$, restituiscono -1 o 1 rispettivamente se il {\it weak learner} $h_i$ fallisce o indovina la predizione su tale immagine\footnote{ricordiamo che gli $h_i$ utilizzati sono classificatori binari, in grado di distinguere una singola cifra da tutte le altre, pertanto restituiranno $-1$ per indicare ``cifra non corrispondente'' e $1$ per indicare ``cifra corrispondente''}.

Si pu\`o dimostrare che minimizzare il training error di un classificatore $h$ equivale a minimizzare la funzione

\begin{equation} \label{eq:maggiorante}
e^{-w_i}(1 - \varepsilon_i) + e^{w_i}\varepsilon_i 
\end{equation}

in cui

\[ \varepsilon_i \eqdef \sum\limits_{t=1}^{|S|}{\mathbb{I}\{L_i(t) = -1\}\mathbb{P}_i(t) } \]

e le $\mathbb{P}_i$ rappresentano le funzioni di probabilit\`a sullo spazio campionario \(\Omega = \{1, \dotsc, m\}\) su cui le funzioni $L_i$ sono variabili casuali. Queste funzioni vengono inizializzate al valore \(P_i(t) = \frac{1}{m} \hskip 0.5em \forall i \) (dove $m$ \`e la dimensione del training set $S$) e ricalcolate iterativamente ad ogni passo di Adaboost tramite la formula

\[ P_{i+1}(t) \eqdef \frac{\mathbb{P}_i(t)e^{-w_iL_i(t)}}{\mathbb{E}_i[e^{-w_iL_i(t)}]} \]

Il minimo dell'equazione \ref{eq:maggiorante} corrisponde a \(w = \frac{1}{2} \ln{\frac{1 - \varepsilon}{\varepsilon}}\), pertanto assegnamo questo valore al peso $w_i$ da dare al {\it weak learner} $h_i$ nella somma \ref{eq:strongclass}.
Nella costruzione di tale somma vogliamo che ogni componente abbia un peso il pi\`u  diverso possibile da $1/2$, poich\'e questo corrisponde ad avere risposte pi\`u ``certe'' da parte dei singoli $h_i$.

Per questa ragione, ad ogni passo Adaboost seleziona il prossimo classificatore $h_i$ da aggiungere alla somma in base al valore dell'$\varepsilon_i$ ad esso corrispondente. Il classificatore per cui \(|\varepsilon_i - 1/2|\) \`e massimo viene scelto.