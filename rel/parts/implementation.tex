

%%%%%% 3: DESCRIZIONE IMPLEMENTAZIONE
\section{Implementazione}
Vista la relativa semplicit\`a dell'algoritmo Adaboost e dei dati trattati, si \`e scelto di implementare la lettura dei dati e l'algoritmo stesso manualmente\footnote{Il codice sorgente \`e disponibile all'URL \url{https://github.com/silverweed/mspa/}}. \`E stato adottato un approccio basato fortemente sulla programmazione a contratto ed \`e stato usato il linguaggio di programmazione D\footnote{https://dlang.org/}.

Il flusso logico del programma realizzato \`e il seguente:
\begin{enumerate}
\item I dati vengono letti da file come descritto nella sezione \ref{sec:data}.
\item Si esegue il training dei 10 {\it strong classifier} (uno per cifra) tramite Adaboost. Il training set usato \`e lo stesso per ognuno.
\item Si calcolano i valori statistici d'interesse per ciascun algoritmo (training error, test error, ...).
\item Si combinano i classificatori in un unico macro-classificatore {\it one-vs-all} e se ne calcolano i valori statistici d'interesse.
\end{enumerate}

Il programma ha inizialmente generato i risultati relativi a \(T = 250\). Dopodich\'e, dal momento che i coefficienti trovati (pesi, pixel e soglie) non dipendono dal T usato, sono stati estratti da questi risultati quelli relativi a \(1 \le T \le 249\), sui quali sono stati ricalcolati i parametri statistici d'interesse. Abbiamo cos\`i ottenuto i test error e la percentuale di {\it guess} sul test set per tutti i classificatori forti formati da un numero variabile di {\it weak learner} compreso tra 1 e 250.
